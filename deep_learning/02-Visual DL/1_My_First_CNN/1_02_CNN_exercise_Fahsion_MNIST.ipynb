{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "1_02_CNN_exercise_Fahsion_MNIST.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZc219MFFibh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# My First Convolutional Neural Network\n",
    "\n",
    "## Fashion MNIST\n",
    "\n",
    "#### 실습목표<br>\n",
    "1. CNN의 기본 아이디어를 안다.\n",
    "2. CNN의 구조를 그리고, 코드로 옮길 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLYle-HPFNgx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Real Game : CNN on Fashion MNIST\n",
    "\n",
    "여기에서는 여러분이 직접 코드를 완성해야 하는 문제가 곳곳에 있습니다!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SpxQtfTUUI5r",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "'''\n",
    "matplolib inline 명령어를 통해서\n",
    "matplot으로 그리는 플롯들을 주피터 노트북 내에서 볼 수 있게 해준다.\n",
    "포맷을 retina로 바꾸면 그래프의 화질이 훨씬 좋아진다.\n",
    "'''\n",
    "# % matplotlib inline\n",
    "# % config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "'''\n",
    "라이브러리들을 불러오자.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import random as rd\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxB_-9itUhFy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4uNApF4YVdf7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.fashion_mnist.load_data()"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0EcfUETOUhIy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D7-ICNlAVhIM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "labels = [\"T-shirt/top\",  # index 0\n",
    "          \"Trouser\",  # index 1\n",
    "          \"Pullover\",  # index 2\n",
    "          \"Dress\",  # index 3\n",
    "          \"Coat\",  # index 4\n",
    "          \"Sandal\",  # index 5\n",
    "          \"Shirt\",  # index 6\n",
    "          \"Sneaker\",  # index 7\n",
    "          \"Bag\",  # index 8\n",
    "          \"Ankle boot\"]  # index 9\n",
    "\n",
    "print(labels)"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "06UwAC3JV7uj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "'''\n",
    "Ctrl+Enter를 이용하여\n",
    "반복 실행 해보자!\n",
    "'''\n",
    "\n",
    "id = rd.randrange(0, 10000)\n",
    "\n",
    "print(f'id = {id}')\n",
    "print(f'다음 그림은 {labels[test_y[id]]} 입니다.')\n",
    "plt.imshow(test_x[id], cmap='Greys')\n",
    "plt.show()"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = 3163\n",
      "다음 그림은 Shirt 입니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAASo0lEQVR4nO3dW2xd5ZUH8P8iiUkc52o7iZM4OISgDCLioiNAIoJAlQjyAn3oqAihDEITHkAqUh8GMQ/lEY2mrSoBFe4ATUcdqoqC4CGaFkUg1BfECYTcTHAmMokTxzYhFzs3knjNgzeVCd5rmbP3OXvD+v8ky/ZZ3ud8OfY/2z5rf98nqgoi+uG7qugBEFFjMOxEQTDsREEw7ERBMOxEQUxv5IO1tbVpV1dXIx8yvM8++8ysX3WV/f/99On2j4iImPVrrrnGrFO++vr68MUXX0z6TckUdhG5D8BvAEwD8F+q+pz19V1dXahWq1ke8gfJa396gbLce++9Zr25udmst7e3m/Wmpiaz/tJLL5l1y9jYmFn3npcsz9v3VaVSSa3V/Gu8iEwD8AKA+wHcAOAhEbmh1vsjovrK8jf7bQAOqOpBVf0KwJ8APJDPsIgob1nCvgzA4Qmf9ye3fYOIbBGRqohUh4eHMzwcEWWRJeyT/UH0rT8+VbVbVSuqWvH+/iOi+skS9n4AnRM+Xw7gaLbhEFG9ZAn7hwBWi8hKEWkC8FMAb+czLCLKW82tN1W9JCJPAvgrxltvr6jq3txGVjJWeyxriyfr8X19fam1BQsWmMe+8cYbmR57w4YNZv2TTz5Jrd10003msd41AFnUs91ZVpn67Kq6DcC2nMZCRHXEy2WJgmDYiYJg2ImCYNiJgmDYiYJg2ImCaOh89jKrZ9/1q6++Muv79u0z6++9955Z//zzz1Nr69evN4/t6ekx6ydPnjTrq1atMuvPP/98am3NmjXmsRs3bjTra9euNeuWH2If3cMzO1EQDDtREAw7URAMO1EQDDtREAw7URBsvSWytGJeeOEFs75jxw6z3traatZXrFhh1m+//fbU2tmzZ81jH3nkEbPutQ3nzZtn1hctWpRaO378uHlsd3e3Wb906ZJZv+OOO1JrmzdvNo/9IeKZnSgIhp0oCIadKAiGnSgIhp0oCIadKAiGnSgI9tmn6PHHH0+tLVmyxDx206ZNmR774sWLZt2anrtw4ULz2FOnTpl1bynq5cuXm/XR0dHUmnd9wV133WXWjx07Ztat6buPPvqoeeyrr75q1r+PeGYnCoJhJwqCYScKgmEnCoJhJwqCYScKgmEnCoJ99sTg4KBZb2lpSa21tbWZx1pLPQNAU1OTWZ89e7ZZnzZtWmrNm4++ePFis3758mWzPjQ0ZNazjO38+fNm3ZvPvmzZstTaiRMnzGN7e3vN+urVq816GWUKu4j0ARgBcBnAJVWt5DEoIspfHmf2e1T1ixzuh4jqiH+zEwWRNewK4G8iskNEtkz2BSKyRUSqIlIdHh7O+HBEVKusYb9TVW8FcD+AJ0TkWzMXVLVbVSuqWmlvb8/4cERUq0xhV9WjyfshAG8CuC2PQRFR/moOu4jMFpE5X38MYCOAPXkNjIjyleXV+MUA3kzWW58O4H9U9X9zGVUBvF641ROeOXOmeaw3331kZMSse71ui9XnBoDTp0+bde8agBkzZpj16dPTf8QuXLhgHtvc3GzWvbFZc/Wvuso+z3lr/Yfqs6vqQQA35TgWIqojtt6IgmDYiYJg2ImCYNiJgmDYiYLgFNfEgQMHzLo1zfTjjz82j73lllvMujV9Fsi2lLTXGvPaX9Z9A37b8dy5c6m1rP9ur753797U2tVXX20e67Viv494ZicKgmEnCoJhJwqCYScKgmEnCoJhJwqCYScKgn32xOHDh8261U8+fvy4eazXs+3o6DDr3rbJyTTj71wD/CmwY2NjZt1b7tmavutdA3D06FGzPn/+fLNu9fiXLl1qHrt//36z/n3EMztREAw7URAMO1EQDDtREAw7URAMO1EQDDtREOyzJ6yeLGDPZ581a5Z5rNcPHh0dNeteP9q6BsDb1rje5syZk1rzlrH25px7S2xby0UvXLjQPHb37t1m3Zvn713fUASe2YmCYNiJgmDYiYJg2ImCYNiJgmDYiYJg2ImCYJ89YW3vCwDt7e2ptRMnTpjHej1db+11a7towN4W2eNdX+D1k71tk63793rR3n17W11bz6u3hoA3trNnz5p167qMorhndhF5RUSGRGTPhNsWisg7ItKbvLdXVyCiwk3l1/jfA7jvitueBrBdVVcD2J58TkQl5oZdVd8H8OUVNz8AYGvy8VYAD+Y7LCLKW60v0C1W1QEASN4vSvtCEdkiIlURqQ4PD9f4cESUVd1fjVfVblWtqGrFepGLiOqr1rAPikgHACTvh/IbEhHVQ61hfxvA5uTjzQDeymc4RFQvboNWRF4DsB5Am4j0A/gFgOcA/FlEHgNwCMBP6jnIRvDWP7f6sl4f/OTJk2Z9+fLlZt2bk56lz15vZ86cSa15fXRvzXrrvgF7Pf8jR46Yx3rXRnz55ZWvWX9TGfvs7k+Jqj6UUvpRzmMhojri5bJEQTDsREEw7ERBMOxEQTDsREGUt2eTM6+15k1ptJYttpYsBvzW29q1a2t+bMBuvXnHNjc3m3VvKqfHWg7a+554j33x4kWzbrW/WltbzWM9g4ODZr2zszPT/dcDz+xEQTDsREEw7ERBMOxEQTDsREEw7ERBMOxEQYTps3tLRXs922PHjqXWvCmue/bsMev33HOPWfd65d7Yi7pvAJg7d25qzdvq2lvZyNvyecWKFak1b9qwN4XVu0agjHhmJwqCYScKgmEnCoJhJwqCYScKgmEnCoJhJwoiTJ/d64tmWfrX61V787JnzJhh1r2esDcX3+Jtyez1wi9cuGDWrT79nDlzzGO979nQkL03ifV98Xr03rUT3vFlxDM7URAMO1EQDDtREAw7URAMO1EQDDtREAw7URBh+uze2u1eL3vJkiWptba2tlqG9A/e9sHels5WL9vbztnrk3tr4nt16xoCr08+c+ZMs97T02PW16xZk1pbvHixeWxLS4tZzzrPvwjumV1EXhGRIRHZM+G2Z0XkiIjsTN421XeYRJTVVH6N/z2A+ya5/deqenPyti3fYRFR3tywq+r7AOw1eoio9LK8QPekiOxKfs1fkPZFIrJFRKoiUh0eHs7wcESURa1h/y2AVQBuBjAA4JdpX6iq3apaUdWKt4AgEdVPTWFX1UFVvayqYwB+B+C2fIdFRHmrKewi0jHh0x8DsNdKJqLCuX12EXkNwHoAbSLSD+AXANaLyM0AFEAfgMfrN8R8eH10r188MjKSWrN68ABw7tw5s37o0CGzvnLlSrM+OjqaWvP67Nb+6YA/V96bD2/NC/ce2/ueeesAWPPlz5w5Yx47f/58s+7Ndy8jN+yq+tAkN79ch7EQUR3xclmiIBh2oiAYdqIgGHaiIBh2oiDCTHE9ceKEWfemelptIK+1tm7dOrPutYG8tqDVwvKmYnqP7bXevCW4reObm5vNY/ft22fWV61aZdbHxsZSa94W3q2trWbdWz68jHhmJwqCYScKgmEnCoJhJwqCYScKgmEnCoJhJwoiTJ/ds2jRIrNu9au9Hv7dd99t1j/99FOz7i2DbU0z9frk3hTYLNtBA0BTU1Nqzduy2at3dHSY9R07dqTWOjs7zWO9paS9bbjLiGd2oiAYdqIgGHaiIBh2oiAYdqIgGHaiIBh2oiDC9NkPHjxo1r251dayxdOmTTOPtXrNgL8ssTd32tra2JrTDfj/bq/P7vWbresTvLnw3tj7+/vNurVGgddH97aL9tYBKCOe2YmCYNiJgmDYiYJg2ImCYNiJgmDYiYJg2ImCCNNn9+Zte2uzWz1bb+tgrw+fZV14wO7Te2Pz5uJ7j+31yq3797Zk9nrd3ve0t7c3teZdX+CN7fvIPbOLSKeIvCsiPSKyV0R+lty+UETeEZHe5P2C+g+XiGo1lV/jLwH4uar+E4A7ADwhIjcAeBrAdlVdDWB78jkRlZQbdlUdUNWPko9HAPQAWAbgAQBbky/bCuDBOo2RiHLwnV6gE5EuALcA+ADAYlUdAMb/QwAw6SJuIrJFRKoiUh0eHs44XCKq1ZTDLiItAP4C4ClVPT3V41S1W1Urqlppb2+vZYxElIMphV1EZmA86H9U1TeSmwdFpCOpdwAYqs8QiSgPbutNxuc4vgygR1V/NaH0NoDNAJ5L3r9VlxHmxGuleG0cqw2UdQrryMiIWT9//rxZt8bmTVH12oJe681jLQft/bu8luSsWbPM+po1a1Jr3jLU+/fvN+srVqww62U0lT77nQAeAbBbRHYmtz2D8ZD/WUQeA3AIwE/qMkIiyoUbdlX9O4C008OP8h0OEdULL5clCoJhJwqCYScKgmEnCoJhJwoizBRXr1/sTfU8depUam3BAnvCn3ffXi/cq1tbNnu9bO/6Ao+3zLV1/+fOnTOP9a5f8La6tpZ79p7T0dFRsz537lyzXkY8sxMFwbATBcGwEwXBsBMFwbATBcGwEwXBsBMFEabP3traatb37dtn1q1VdryebJa58oDdRwfs5aK9sXlzxrPOh7f+7V4f3VvGzFsnwLp/71hvu2jv2ooy4pmdKAiGnSgIhp0oCIadKAiGnSgIhp0oCIadKIgwffYbb7zRrL/++utm3eqze3PlvTnl3pr21nbRgN3L9rZsPn78uFlvaWkx6971C6dPp28e5B3b399v1r3n7ciRI6m1W2+91TzWW8v/2muvNetlxDM7URAMO1EQDDtREAw7URAMO1EQDDtREAw7URBT2Z+9E8AfACwBMAagW1V/IyLPAvhXAF9POn5GVbfVa6BZdXV1mXWvn2z1XTdu3Ggea/XoAeDhhx826+vWrTPr1nz4ixcvmsd689k91trsgL2Hujef/dixY2b9uuuuM+tPPfVUam3bNvtH1ZvHv3TpUrNeRlO5qOYSgJ+r6kciMgfADhF5J6n9WlX/s37DI6K8TGV/9gEAA8nHIyLSA2BZvQdGRPn6Tr/DiUgXgFsAfJDc9KSI7BKRV0Rk0nV6RGSLiFRFpOotM0RE9TPlsItIC4C/AHhKVU8D+C2AVQBuxviZ/5eTHaeq3apaUdWK97crEdXPlMIuIjMwHvQ/quobAKCqg6p6WVXHAPwOwG31GyYRZeWGXcZflnwZQI+q/mrC7R0TvuzHAPbkPzwiystUXo2/E8AjAHaLyM7ktmcAPCQiNwNQAH0AHq/D+HJz9uxZs97T02PWremYVnsJAJ5++mmzvmHDBrP+7rvvmvWVK1em1ryW4sDAgFn3WlBZW3eWwcFBs/7iiy+a9Q8++CC1tmvXLvPYw4cPm3Vvem0ZTeXV+L8DmOw7XtqeOhF9G6+gIwqCYScKgmEnCoJhJwqCYScKgmEnCkK87YDzVKlUtFqtNuzxvotDhw6ZdWu55nnz5pnHzp49u6Yxfc1b1ri3tze1Nn/+fPPYvr4+s97c3GzWrR4/YI/d69FnXa7Z6oV7PXxv6u71119f05jqrVKpoFqtTnpxBM/sREEw7ERBMOxEQTDsREEw7ERBMOxEQTDsREE0tM8uIsMAPp9wUxuALxo2gO+mrGMr67gAjq1WeY7tGlWddP23hob9Ww8uUlXVSmEDMJR1bGUdF8Cx1apRY+Ov8URBMOxEQRQd9u6CH99S1rGVdVwAx1arhoyt0L/Ziahxij6zE1GDMOxEQRQSdhG5T0T2i8gBEbEXVW8wEekTkd0islNECp18n+yhNyQieybctlBE3hGR3uT9pHvsFTS2Z0XkSPLc7RSRTQWNrVNE3hWRHhHZKyI/S24v9LkzxtWQ563hf7OLyDQAnwHYAKAfwIcAHlLVfQ0dSAoR6QNQUdXCL8AQkbsAjAL4g6remNz2HwC+VNXnkv8oF6jqv5VkbM8CGC16G+9kt6KOiduMA3gQwL+gwOfOGNc/owHPWxFn9tsAHFDVg6r6FYA/AXiggHGUnqq+D+DLK25+AMDW5OOtGP9habiUsZWCqg6o6kfJxyMAvt5mvNDnzhhXQxQR9mUAJu6t049y7feuAP4mIjtEZEvRg5nEYlUdAMZ/eAAsKng8V3K38W6kK7YZL81zV8v251kVEfbJ1scqU//vTlW9FcD9AJ5Ifl2lqZnSNt6NMsk246VQ6/bnWRUR9n4AnRM+Xw7gaAHjmJSqHk3eDwF4E+Xbinrw6x10k/dDBY/nH8q0jfdk24yjBM9dkdufFxH2DwGsFpGVItIE4KcA3i5gHN8iIrOTF04gIrMBbET5tqJ+G8Dm5OPNAN4qcCzfUJZtvNO2GUfBz13h25+rasPfAGzC+Cvy/wfg34sYQ8q4rgXwSfK2t+ixAXgN47/WXcT4b0SPAWgFsB1Ab/J+YYnG9t8AdgPYhfFgdRQ0tnUY/9NwF4Cdydumop87Y1wNed54uSxRELyCjigIhp0oCIadKAiGnSgIhp0oCIadKAiGnSiI/wePwSbxUdljGwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghiwVWATe4I8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### X : Min-Max Scaling\n",
    "\n",
    "1. 최소값 0, 최대값 1로 통일하는 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6kyCJdobSPGV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XotXiXZYfNOM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### X : Reshape(# of data, 28, 28, 1)\n",
    "\n",
    "**끝에 1을 달아서 그레이스케일(흑백)을 명시해준다.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Emex2pYHfNId",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train_x = train_x.reshape(-1, 28, 28, 1)\n",
    "test_x = test_x.reshape(-1, 28, 28, 1)"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyVgGB6lfCuj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Y : One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZRDNj4XUfLhm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from keras.utils import to_categorical"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_class = len(np.unique(train_y))\n",
    "n_class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_y, n_class)\n",
    "test_y = to_categorical(test_y, n_class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ_FTTj7XAtC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 코드를 완성해주세요!\n",
    "\n",
    "**자유롭게 먼저 해보는 것을 추천**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**구조를 따라서 코딩을 한다면..**\n",
    "\n",
    "1. 인풋레이어\n",
    "1. Convolution : 필터수 32개, 사이즈(3, 3), same padding\n",
    "2. BatchNormalization\n",
    "2. Convolution : 필터수 32개, 사이즈(3, 3), same padding\n",
    "3. BatchNormalization\n",
    "4. MaxPooling : 사이즈(2,2) 스트라이드(2,2)\n",
    "5. DropOut : 25% 비활성화\n",
    "1. Convolution : 필터수 64개, 사이즈(3, 3), same padding\n",
    "3. BatchNormalization\n",
    "2. Convolution : 필터수 64개, 사이즈(3, 3), same padding\n",
    "3. BatchNormalization\n",
    "4. MaxPooling : 사이즈(2,2) 스트라이드(2,2)\n",
    "5. DropOut : 25% 비활성화\n",
    "6. Flatten()\n",
    "7. Fully Connected Layer : 노드 512개\n",
    "3. BatchNormalization\n",
    "9. 아웃풋레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.callbacks import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# clear session\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# 모델 선언\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Input(shape=train_x.shape[1:]))\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', strides=(1, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', strides=(1, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), (2, 2)))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu', strides=(1, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu', strides=(1, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "HQ34k0qHsP0_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,679,082\n",
      "Trainable params: 1,677,674\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNeCsx1MelTx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Early Stopping을 활용한 학습\n",
    "\n",
    "1. validation_split = 0.2\n",
    "2. 1 epochs만 관찰해가며 속도가 가장 빠른 batch_size 찾아보기. 128개부터 시작하여 조절해볼 것.\n",
    "3. EarlyStopping. val_loss가 4 epoch 전과 비교하여 개선되지 않으면 스탑\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   min_delta=0,\n",
    "                   patience=5,\n",
    "                   verbose=1,\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "mcp = ModelCheckpoint(filepath='./content/CNN_Fashion.ckpt',\n",
    "                      monitor='val_loss',\n",
    "                      save_best_only=True,\n",
    "                      save_weights_only=True,\n",
    "                      verbose=1)"
   ],
   "metadata": {
    "id": "Cl-0is0SyGC7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "   1/1500 [..............................] - ETA: 0s - loss: 2.3021 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 0.0210s). Check your callbacks.\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0993\n",
      "Epoch 00001: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 2.3031 - accuracy: 0.0993 - val_loss: 2.3031 - val_accuracy: 0.0983\n",
      "Epoch 2/1000\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.1008\n",
      "Epoch 00002: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 2.3031 - accuracy: 0.1009 - val_loss: 2.3032 - val_accuracy: 0.0957\n",
      "Epoch 3/1000\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 2.3032 - accuracy: 0.0980\n",
      "Epoch 00003: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3032 - accuracy: 0.0980 - val_loss: 2.3033 - val_accuracy: 0.1013\n",
      "Epoch 4/1000\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 2.3030 - accuracy: 0.1007\n",
      "Epoch 00004: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3030 - accuracy: 0.1004 - val_loss: 2.3029 - val_accuracy: 0.0983\n",
      "Epoch 5/1000\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0996\n",
      "Epoch 00005: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.1000 - val_loss: 2.3039 - val_accuracy: 0.0983\n",
      "Epoch 6/1000\n",
      "1500/1500 [==============================] - ETA: 0s - loss: 2.3031 - accuracy: 0.0993\n",
      "Epoch 00006: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.0993 - val_loss: 2.3033 - val_accuracy: 0.1030\n",
      "Epoch 7/1000\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0986\n",
      "Epoch 00007: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.0987 - val_loss: 2.3033 - val_accuracy: 0.1027\n",
      "Epoch 8/1000\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.1001\n",
      "Epoch 00008: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.1003 - val_loss: 2.3030 - val_accuracy: 0.0957\n",
      "Epoch 9/1000\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0985Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.0985 - val_loss: 2.3030 - val_accuracy: 0.0957\n",
      "Epoch 00009: early stopping\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3028 - accuracy: 0.1000\n",
      "batch_size32: [2.3028314113616943, 0.10000000149011612]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3028 - accuracy: 0.1000\n",
      "Epoch 1/1000\n",
      "740/750 [============================>.] - ETA: 0s - loss: 2.3029 - accuracy: 0.0992\n",
      "Epoch 00001: val_loss improved from 2.30267 to 2.30256, saving model to ./content\\CNN_Fashion.ckpt\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 2.3029 - accuracy: 0.0992 - val_loss: 2.3026 - val_accuracy: 0.1013\n",
      "Epoch 2/1000\n",
      "735/750 [============================>.] - ETA: 0s - loss: 2.3028 - accuracy: 0.0995\n",
      "Epoch 00002: val_loss did not improve from 2.30256\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 2.3028 - accuracy: 0.0994 - val_loss: 2.3028 - val_accuracy: 0.0957\n",
      "Epoch 3/1000\n",
      "742/750 [============================>.] - ETA: 0s - loss: 2.3029 - accuracy: 0.0995\n",
      "Epoch 00003: val_loss did not improve from 2.30256\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 2.3029 - accuracy: 0.0995 - val_loss: 2.3027 - val_accuracy: 0.0983\n",
      "Epoch 4/1000\n",
      "745/750 [============================>.] - ETA: 0s - loss: 2.3029 - accuracy: 0.0971\n",
      "Epoch 00004: val_loss did not improve from 2.30256\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 2.3029 - accuracy: 0.0970 - val_loss: 2.3027 - val_accuracy: 0.1005\n",
      "Epoch 5/1000\n",
      "740/750 [============================>.] - ETA: 0s - loss: 2.3030 - accuracy: 0.0972\n",
      "Epoch 00005: val_loss did not improve from 2.30256\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 2.3030 - accuracy: 0.0970 - val_loss: 2.3028 - val_accuracy: 0.0983\n",
      "Epoch 6/1000\n",
      "744/750 [============================>.] - ETA: 0s - loss: 2.3030 - accuracy: 0.0980Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.30256\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 2.3030 - accuracy: 0.0981 - val_loss: 2.3028 - val_accuracy: 0.0957\n",
      "Epoch 00006: early stopping\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "batch_size64: [2.302671432495117, 0.10000000149011612]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3027 - accuracy: 0.1000\n",
      "Epoch 1/1000\n",
      "375/375 [==============================] - ETA: 0s - loss: 2.3028 - accuracy: 0.0992\n",
      "Epoch 00001: val_loss did not improve from 2.30256\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 2.3028 - accuracy: 0.0992 - val_loss: 2.3027 - val_accuracy: 0.1003\n",
      "Epoch 2/1000\n",
      "372/375 [============================>.] - ETA: 0s - loss: 2.3027 - accuracy: 0.1000\n",
      "Epoch 00002: val_loss did not improve from 2.30256\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.1001 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 3/1000\n",
      "364/375 [============================>.] - ETA: 0s - loss: 2.3028 - accuracy: 0.0984\n",
      "Epoch 00003: val_loss did not improve from 2.30256\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0984 - val_loss: 2.3029 - val_accuracy: 0.0995\n",
      "Epoch 4/1000\n",
      "364/375 [============================>.] - ETA: 0s - loss: 2.3028 - accuracy: 0.0983\n",
      "Epoch 00004: val_loss did not improve from 2.30256\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 2.3027 - accuracy: 0.0990 - val_loss: 2.3033 - val_accuracy: 0.0957\n",
      "Epoch 5/1000\n",
      "374/375 [============================>.] - ETA: 0s - loss: 2.3028 - accuracy: 0.0981\n",
      "Epoch 00005: val_loss did not improve from 2.30256\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0981 - val_loss: 2.3028 - val_accuracy: 0.0983\n",
      "Epoch 6/1000\n",
      "363/375 [============================>.] - ETA: 0s - loss: 2.3028 - accuracy: 0.0997Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.30256\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 2.3028 - accuracy: 0.0995 - val_loss: 2.3030 - val_accuracy: 0.0995\n",
      "Epoch 00006: early stopping\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "batch_size128: [2.302607536315918, 0.10000000149011612]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 1/1000\n",
      "180/188 [===========================>..] - ETA: 0s - loss: 2.3027 - accuracy: 0.0995\n",
      "Epoch 00001: val_loss did not improve from 2.30256\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 2.3027 - accuracy: 0.0999 - val_loss: 2.3030 - val_accuracy: 0.0957\n",
      "Epoch 2/1000\n",
      "188/188 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.0990\n",
      "Epoch 00002: val_loss did not improve from 2.30256\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 2.3027 - accuracy: 0.0990 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 3/1000\n",
      "181/188 [===========================>..] - ETA: 0s - loss: 2.3027 - accuracy: 0.0990\n",
      "Epoch 00003: val_loss did not improve from 2.30256\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 2.3027 - accuracy: 0.0985 - val_loss: 2.3027 - val_accuracy: 0.1005\n",
      "Epoch 4/1000\n",
      "181/188 [===========================>..] - ETA: 0s - loss: 2.3027 - accuracy: 0.0994\n",
      "Epoch 00004: val_loss did not improve from 2.30256\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 2.3027 - accuracy: 0.0993 - val_loss: 2.3028 - val_accuracy: 0.0983\n",
      "Epoch 5/1000\n",
      "180/188 [===========================>..] - ETA: 0s - loss: 2.3026 - accuracy: 0.1003\n",
      "Epoch 00005: val_loss did not improve from 2.30256\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 2.3027 - accuracy: 0.1005 - val_loss: 2.3027 - val_accuracy: 0.0983\n",
      "Epoch 6/1000\n",
      "185/188 [============================>.] - ETA: 0s - loss: 2.3027 - accuracy: 0.0977\n",
      "Epoch 00006: val_loss did not improve from 2.30256\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 2.3027 - accuracy: 0.0975 - val_loss: 2.3028 - val_accuracy: 0.0957\n",
      "Epoch 7/1000\n",
      "188/188 [==============================] - ETA: 0s - loss: 2.3027 - accuracy: 0.1010\n",
      "Epoch 00007: val_loss did not improve from 2.30256\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 2.3027 - accuracy: 0.1010 - val_loss: 2.3027 - val_accuracy: 0.1003\n",
      "Epoch 8/1000\n",
      "185/188 [============================>.] - ETA: 0s - loss: 2.3027 - accuracy: 0.1003Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.30256\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 2.3027 - accuracy: 0.1004 - val_loss: 2.3030 - val_accuracy: 0.0957\n",
      "Epoch 00008: early stopping\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "batch_size256: [2.302597999572754, 0.10000000149011612]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 1/1000\n",
      "94/94 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.0998\n",
      "Epoch 00001: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 13ms/step - loss: 2.3026 - accuracy: 0.0998 - val_loss: 2.3028 - val_accuracy: 0.0957\n",
      "Epoch 2/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.0992\n",
      "Epoch 00002: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.0994 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 3/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.0991\n",
      "Epoch 00003: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.0991 - val_loss: 2.3027 - val_accuracy: 0.0995\n",
      "Epoch 4/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1016\n",
      "Epoch 00004: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.1014 - val_loss: 2.3028 - val_accuracy: 0.0983\n",
      "Epoch 5/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3027 - accuracy: 0.1001\n",
      "Epoch 00005: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3027 - accuracy: 0.1003 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 6/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.0993\n",
      "Epoch 00006: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.0994 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 7/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.0993\n",
      "Epoch 00007: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.0992 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 8/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1011\n",
      "Epoch 00008: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.1011 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 9/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1011\n",
      "Epoch 00009: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.1011 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 10/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.0989\n",
      "Epoch 00010: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.0990 - val_loss: 2.3028 - val_accuracy: 0.0957\n",
      "Epoch 11/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1020\n",
      "Epoch 00011: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.1023 - val_loss: 2.3026 - val_accuracy: 0.0983\n",
      "Epoch 12/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1021\n",
      "Epoch 00012: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.1021 - val_loss: 2.3030 - val_accuracy: 0.0983\n",
      "Epoch 13/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3027 - accuracy: 0.0992\n",
      "Epoch 00013: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3027 - accuracy: 0.0991 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 14/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1021\n",
      "Epoch 00014: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.1019 - val_loss: 2.3029 - val_accuracy: 0.1003\n",
      "Epoch 15/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1002\n",
      "Epoch 00015: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3026 - accuracy: 0.1004 - val_loss: 2.3027 - val_accuracy: 0.1005\n",
      "Epoch 16/1000\n",
      "91/94 [============================>.] - ETA: 0s - loss: 2.3027 - accuracy: 0.0983Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 2.30256\n",
      "94/94 [==============================] - 1s 12ms/step - loss: 2.3027 - accuracy: 0.0980 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 00016: early stopping\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "batch_size512: [2.3025991916656494, 0.10000000149011612]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "Epoch 1/1000\n",
      "47/47 [==============================] - ETA: 0s - loss: 2.3026 - accuracy: 0.1003\n",
      "Epoch 00001: val_loss did not improve from 2.30256\n",
      "47/47 [==============================] - 1s 25ms/step - loss: 2.3026 - accuracy: 0.1003 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 2/1000\n",
      "46/47 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.0989\n",
      "Epoch 00002: val_loss did not improve from 2.30256\n",
      "47/47 [==============================] - 1s 21ms/step - loss: 2.3026 - accuracy: 0.0989 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 3/1000\n",
      "46/47 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1013\n",
      "Epoch 00003: val_loss did not improve from 2.30256\n",
      "47/47 [==============================] - 1s 21ms/step - loss: 2.3026 - accuracy: 0.1011 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 4/1000\n",
      "46/47 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1012\n",
      "Epoch 00004: val_loss did not improve from 2.30256\n",
      "47/47 [==============================] - 1s 20ms/step - loss: 2.3026 - accuracy: 0.1011 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 5/1000\n",
      "46/47 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1011\n",
      "Epoch 00005: val_loss did not improve from 2.30256\n",
      "47/47 [==============================] - 1s 20ms/step - loss: 2.3026 - accuracy: 0.1011 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 6/1000\n",
      "46/47 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.0994\n",
      "Epoch 00006: val_loss did not improve from 2.30256\n",
      "47/47 [==============================] - 1s 21ms/step - loss: 2.3026 - accuracy: 0.0994 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 7/1000\n",
      "46/47 [============================>.] - ETA: 0s - loss: 2.3026 - accuracy: 0.1010Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.30256\n",
      "47/47 [==============================] - 1s 21ms/step - loss: 2.3026 - accuracy: 0.1011 - val_loss: 2.3027 - val_accuracy: 0.0957\n",
      "Epoch 00007: early stopping\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.1000\n",
      "batch_size1024: [2.302591562271118, 0.10000000149011612]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in [32, 64, 128, 256, 512, 1024]:\n",
    "    model.fit(train_x, train_y, verbose=1, validation_split=.2, epochs=1000, batch_size=i, callbacks=[es, mcp])\n",
    "    print(f'batch_size{i}: {model.evaluate(test_x, test_y)}')\n",
    "    result.append((i, model.evaluate(test_x, test_y)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3026 - accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": "[2.302611827850342, 0.10000000149011612]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}