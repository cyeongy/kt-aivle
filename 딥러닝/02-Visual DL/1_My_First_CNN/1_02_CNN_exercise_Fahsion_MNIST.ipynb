{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "1_02_CNN_exercise_Fahsion_MNIST.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZc219MFFibh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# My First Convolutional Neural Network\n",
    "\n",
    "## Fashion MNIST\n",
    "\n",
    "#### 실습목표<br>\n",
    "1. CNN의 기본 아이디어를 안다.\n",
    "2. CNN의 구조를 그리고, 코드로 옮길 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLYle-HPFNgx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Real Game : CNN on Fashion MNIST\n",
    "\n",
    "여기에서는 여러분이 직접 코드를 완성해야 하는 문제가 곳곳에 있습니다!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SpxQtfTUUI5r",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "'''\n",
    "matplolib inline 명령어를 통해서\n",
    "matplot으로 그리는 플롯들을 주피터 노트북 내에서 볼 수 있게 해준다.\n",
    "포맷을 retina로 바꾸면 그래프의 화질이 훨씬 좋아진다.\n",
    "'''\n",
    "% matplotlib inline\n",
    "% config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "'''\n",
    "라이브러리들을 불러오자.\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import random as rd\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxB_-9itUhFy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4uNApF4YVdf7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.fashion_mnist.load_data()"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0EcfUETOUhIy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D7-ICNlAVhIM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "labels = [\"T-shirt/top\",  # index 0\n",
    "          \"Trouser\",  # index 1\n",
    "          \"Pullover\",  # index 2\n",
    "          \"Dress\",  # index 3\n",
    "          \"Coat\",  # index 4\n",
    "          \"Sandal\",  # index 5\n",
    "          \"Shirt\",  # index 6\n",
    "          \"Sneaker\",  # index 7\n",
    "          \"Bag\",  # index 8\n",
    "          \"Ankle boot\"]  # index 9\n",
    "\n",
    "print(labels)"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "06UwAC3JV7uj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "'''\n",
    "Ctrl+Enter를 이용하여\n",
    "반복 실행 해보자!\n",
    "'''\n",
    "\n",
    "id = rd.randrange(0, 10000)\n",
    "\n",
    "print(f'id = {id}')\n",
    "print(f'다음 그림은 {labels[test_y[id]]} 입니다.')\n",
    "plt.imshow(test_x[id], cmap='Greys')\n",
    "plt.show()"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = 181\n",
      "다음 그림은 Ankle boot 입니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 251,
       "height": 248
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghiwVWATe4I8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### X : Min-Max Scaling\n",
    "\n",
    "1. 최소값 0, 최대값 1로 통일하는 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6kyCJdobSPGV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_x = train_x / 255\n",
    "test_x = test_x / 255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XotXiXZYfNOM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### X : Reshape(# of data, 28, 28, 1)\n",
    "\n",
    "**끝에 1을 달아서 그레이스케일(흑백)을 명시해준다.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Emex2pYHfNId",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train_x = train_x.reshape(-1, 28, 28, 1)\n",
    "test_x = test_x.reshape(-1, 28, 28, 1)"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyVgGB6lfCuj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Y : One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZRDNj4XUfLhm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from keras.utils import to_categorical"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_class = len(np.unique(train_y))\n",
    "n_class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_y, n_class)\n",
    "test_y = to_categorical(test_y, n_class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ_FTTj7XAtC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 코드를 완성해주세요!\n",
    "\n",
    "**자유롭게 먼저 해보는 것을 추천**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**구조를 따라서 코딩을 한다면..**\n",
    "\n",
    "1. 인풋레이어\n",
    "1. Convolution : 필터수 32개, 사이즈(3, 3), same padding\n",
    "2. BatchNormalization\n",
    "2. Convolution : 필터수 32개, 사이즈(3, 3), same padding\n",
    "3. BatchNormalization\n",
    "4. MaxPooling : 사이즈(2,2) 스트라이드(2,2)\n",
    "5. DropOut : 25% 비활성화\n",
    "1. Convolution : 필터수 64개, 사이즈(3, 3), same padding\n",
    "3. BatchNormalization\n",
    "2. Convolution : 필터수 64개, 사이즈(3, 3), same padding\n",
    "3. BatchNormalization\n",
    "4. MaxPooling : 사이즈(2,2) 스트라이드(2,2)\n",
    "5. DropOut : 25% 비활성화\n",
    "6. Flatten()\n",
    "7. Fully Connected Layer : 노드 512개\n",
    "3. BatchNormalization\n",
    "9. 아웃풋레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.callbacks import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# clear session\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# 모델 선언\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(Input(shape=train_x.shape[1:]))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), (2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), (2, 2)))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "id": "HQ34k0qHsP0_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,642,026\n",
      "Trainable params: 1,640,682\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNeCsx1MelTx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Early Stopping을 활용한 학습\n",
    "\n",
    "1. validation_split = 0.2\n",
    "2. 1 epochs만 관찰해가며 속도가 가장 빠른 batch_size 찾아보기. 128개부터 시작하여 조절해볼 것.\n",
    "3. EarlyStopping. val_loss가 4 epoch 전과 비교하여 개선되지 않으면 스탑\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   min_delta=0,\n",
    "                   patience=5,\n",
    "                   verbose=1,\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "mcp = ModelCheckpoint(filepath='./content/CNN_Fashion.ckpt',\n",
    "                      monitor='val_loss',\n",
    "                      save_best_only=True,\n",
    "                      save_weights_only=True,\n",
    "                      verbose=1)"
   ],
   "metadata": {
    "id": "Cl-0is0SyGC7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "   1/1500 [..............................] - ETA: 0s - loss: 2.3026 - accuracy: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0090s). Check your callbacks.\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 2.3034 - accuracy: 0.0995\n",
      "Epoch 00001: val_loss improved from inf to 2.30395, saving model to ./contetn\\CNN_Fashion.ckpt\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 2.3034 - accuracy: 0.0996 - val_loss: 2.3039 - val_accuracy: 0.1005\n",
      "Epoch 2/128\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 2.3032 - accuracy: 0.0950\n",
      "Epoch 00002: val_loss improved from 2.30395 to 2.30294, saving model to ./contetn\\CNN_Fashion.ckpt\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3032 - accuracy: 0.0950 - val_loss: 2.3029 - val_accuracy: 0.0957\n",
      "Epoch 3/128\n",
      "1478/1500 [============================>.] - ETA: 0s - loss: 2.3030 - accuracy: 0.0987\n",
      "Epoch 00003: val_loss did not improve from 2.30294\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3030 - accuracy: 0.0989 - val_loss: 2.3038 - val_accuracy: 0.0957\n",
      "Epoch 4/128\n",
      "1473/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0998\n",
      "Epoch 00004: val_loss did not improve from 2.30294\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.1001 - val_loss: 2.3033 - val_accuracy: 0.0983\n",
      "Epoch 5/128\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.1003\n",
      "Epoch 00005: val_loss improved from 2.30294 to 2.30267, saving model to ./contetn\\CNN_Fashion.ckpt\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.1002 - val_loss: 2.3027 - val_accuracy: 0.1003\n",
      "Epoch 6/128\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0981\n",
      "Epoch 00006: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.0980 - val_loss: 2.3027 - val_accuracy: 0.1030\n",
      "Epoch 7/128\n",
      "1483/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0987\n",
      "Epoch 00007: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.0987 - val_loss: 2.3030 - val_accuracy: 0.0983\n",
      "Epoch 8/128\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0979\n",
      "Epoch 00008: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.0980 - val_loss: 2.3028 - val_accuracy: 0.1003\n",
      "Epoch 9/128\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0996\n",
      "Epoch 00009: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.0995 - val_loss: 2.3028 - val_accuracy: 0.0983\n",
      "Epoch 10/128\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 2.3031 - accuracy: 0.0986Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.30267\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 2.3031 - accuracy: 0.0988 - val_loss: 2.3036 - val_accuracy: 0.1027\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x27c96401c70>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, verbose=1, validation_split=.2, epochs=128, callbacks=[es, mcp])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 2.3027 - accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": "[2.3027243614196777, 0.10000000149011612]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}